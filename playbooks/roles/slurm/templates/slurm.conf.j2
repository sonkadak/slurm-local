#
# Example slurm.conf file generated by DeepOps. 
#
# Slurm provides configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
# See the slurm.conf man page for more information.

# Define a name for the cluster
ClusterName={{ slurm_cluster_name }}

# Configure the controllers
SlurmctldHost={{ slurm_controller_hostname }}
StateSaveLocation=/var/spool/slurm/ctld

# Basic configuration
SlurmUser={{ slurm_user }}
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
PluginDir=/usr/local/lib/slurm
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
PropagateResourceLimitsExcept=MEMLOCK

# Basic job behavior
ReturnToService=1
RebootProgram="/bin/systemctl reboot"
ResumeTimeout=900

# Use PMIX as our default MPI configuration
MpiDefault=pmix

# Prolog/epilog config
#PrologFlags=Alloc,Serial,Contain
#Prolog=/etc/slurm/prolog.sh
#Epilog=/etc/slurm/epilog.sh
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=

# Health checking
HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=300
HealthCheckNodeState=IDLE

# Mail program to use
MailProg=/usr/bin/s-nail

TaskPlugin=affinity,cgroup
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=

# TIMERS
SlurmctldTimeout=120
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0

# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=

# ACCOUNTING
JobAcctGatherType=jobacct_gather/cgroup
#JobAcctGatherFrequency=30
AccountingStorageTRES=gres/gpu
#DebugFlags=CPU_Bind,gres
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ slurm_controller_hostname }}
#AccountingStorageLoc=
#AccountingStorageEnforce=associations,limits,qos
AccountingStorageUser=slurm
AccountingStoragePass=/var/run/munge/munge.socket.2

# COMPUTE NODES
GresTypes=gpu

{% for node_name in groups['compute'] %}
{% set memory =  hostvars[node_name]["ansible_local"]["memory"] -%}
{% set cpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["cpu_topology"] -%}
{% set gpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["gpu_topology"] -%}
    NodeName={{ node_name }}{{ " " -}}
    {%- if gpu_topology|count %} Gres=gpu:{{ gpu_topology|count }} {% endif -%}
    CPUs={{ cpu_topology.logical_cpus|int }}{{ " " -}}
    Sockets={{ cpu_topology.sockets|int }}{{ " " -}}
    CoresPerSocket={{ cpu_topology.cores_per_socket|int }}{{ " " -}}
    ThreadsPerCore={{ cpu_topology.threads_per_core }}{{ " " -}}
    Procs={{ cpu_topology.sockets|int * cpu_topology.cores_per_socket|int }}{{ " " -}}
    RealMemory={{ memory.total_mb|int }}{{ " " -}}
    State=UNKNOWN
{% endfor %}

PartitionName={{ slurm_partition_name }} Nodes=ALL Default=YES DefMemPerCPU=0 State=UP OverSubscribe=NO MaxTime=INFINITE
